<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Contribution to Wiley StatsRef</title>
  <meta name="description" content="This project contains two contributions to Wiley StatsRef" />
  <meta name="generator" content="bookdown 0.21 and GitBook 2.6.7" />

  <meta property="og:title" content="Contribution to Wiley StatsRef" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This project contains two contributions to Wiley StatsRef" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Contribution to Wiley StatsRef" />
  
  <meta name="twitter:description" content="This project contains two contributions to Wiley StatsRef" />
  

<meta name="author" content="Stef van Buuren" />


<meta name="date" content="2020-11-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  

<link rel="next" href="references.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/accessible-code-block-0.0.1/empty-anchor.js"></script>
<link href="libs/anchor-sections-1.0/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0/anchor-sections.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html"><i class="fa fa-check"></i>MICE - Multivariate Imputation by Chained Equations</a><ul>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#historic-background"><i class="fa fa-check"></i>Historic background</a></li>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#multiple-imputation"><i class="fa fa-check"></i>Multiple imputation</a></li>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#practical-problems-in-multivariate-imputation"><i class="fa fa-check"></i>Practical problems in multivariate imputation</a></li>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#sec:MICE"><i class="fa fa-check"></i>The MICE algorithm</a></li>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#methodology"><i class="fa fa-check"></i>Methodology</a><ul>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#mcmc-conditions"><i class="fa fa-check"></i>MCMC Conditions</a></li>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#compatibility"><i class="fa fa-check"></i>Compatibility</a></li>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#sec:howlarget"><i class="fa fa-check"></i>Number of iterations</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#performance"><i class="fa fa-check"></i>Performance</a></li>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#future-work"><i class="fa fa-check"></i>Future work</a><ul>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#skipping-imputations-and-overimputation"><i class="fa fa-check"></i>Skipping imputations and overimputation</a></li>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#sec:blockvar"><i class="fa fa-check"></i>Blocks of variables, hybrid imputation</a></li>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#sec:blockunit"><i class="fa fa-check"></i>Blocks of units, monotone blocks</a></li>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#separate-training-from-test-data"><i class="fa fa-check"></i>Separate training from test data</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="mice-multivariate-imputation-by-chained-equations.html"><a href="mice-multivariate-imputation-by-chained-equations.html#conclusion"><i class="fa fa-check"></i>Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Contribution to Wiley StatsRef</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="header">
<h1 class="title">Contribution to Wiley StatsRef</h1>
<p class="author"><em>Stef van Buuren</em></p>
<p class="date"><em>2020-11-12</em></p>
</div>
<div id="mice---multivariate-imputation-by-chained-equations" class="section level1">
<h1>MICE - Multivariate Imputation by Chained Equations</h1>
<p>Multivariate Imputation by Chained Equations (MICE) is an algorithm to create synthetic values (imputations) for multivariate missing data. This article briefly reviews ideas similar to MICE, explains the difference between single and multiple imputation and highlights practical problems in multivariate imputation. The MICE algorithm iteratively imputes the data variable-by-variable. The text discusses the conditions needed for convergence, the issues of compatibility between the complete-data model and the imputation model, the number of iterations, the performance of the algorithm, and potential extensions.</p>
<div id="historic-background" class="section level2">
<h2>Historic background</h2>
<p>MICE is an acronym for <em>Multivariate Imputation by Chained Equations</em>. The term MICE refers to an algorithm to impute multivariate missing data. The user specifies the distribution of the missing data in each incomplete variable conditional on other data. For example, we could use logistic regression to impute incomplete binary variables, polytomous regression for categorical data, and linear regression for numerical data. The MICE algorithm generates multiple imputations by iteratively drawing values from these conditional distributions. The algorithm was first published as S-PLUS software <span class="citation">(van Buuren and Groothuis-Oudshoorn <a href="#ref-VANBUUREN1999B" role="doc-biblioref">1999</a>)</span>. In 2006 it became widely available as an R package on CRAN <span class="citation">(van Buuren <a href="#ref-VANBUUREN2011" role="doc-biblioref">2011</a>)</span>. SAS 9.3, SPSS 17.0 and Stata 12 introduced versions of the MICE algorithm in their offerings.</p>
<p>Ideas similar to MICE have surfaced under other names: stochastic relaxation <span class="citation">(Kennickell <a href="#ref-KENNICKELL1991" role="doc-biblioref">1991</a>)</span>, variable-by-variable imputation <span class="citation">(Brand <a href="#ref-BRAND1999" role="doc-biblioref">1999</a>)</span>, switching regressions <span class="citation">(van Buuren, Boshuizen, and Knook <a href="#ref-VANBUUREN1999" role="doc-biblioref">1999</a>)</span>, sequential regressions <span class="citation">(Raghunathan et al. <a href="#ref-RAGHUNATHAN2001" role="doc-biblioref">2001</a>)</span>, ordered pseudo-Gibbs sampler <span class="citation">(Heckerman et al. <a href="#ref-HECKERMAN2001" role="doc-biblioref">2001</a>)</span>, partially incompatible MCMC <span class="citation">(Rubin <a href="#ref-RUBIN2003" role="doc-biblioref">2003</a>)</span>, iterated univariate imputation <span class="citation">(Gelman <a href="#ref-GELMAN2004" role="doc-biblioref">2004</a>)</span>, chained equations <span class="citation">(van Buuren and Groothuis-Oudshoorn <a href="#ref-VANBUUREN1999B" role="doc-biblioref">1999</a>)</span> and fully conditional specification (FCS) <span class="citation">(van Buuren et al. <a href="#ref-VANBUUREN2006" role="doc-biblioref">2006</a>)</span>. A simple Google search reveals that “chained equations” has become the most popular name.</p>
</div>
<div id="multiple-imputation" class="section level2">
<h2>Multiple imputation</h2>
<p>Multiple imputation <span class="citation">(Rubin <a href="#ref-RUBIN1987" role="doc-biblioref">1987</a>)</span> is a general method to deal with incomplete data. Many analysts attempt to replace a missing entry by the “best” value according to some prediction method, a strategy known as <em>single imputation</em>. However, standard errors, confidence intervals and <span class="math inline">\(P\)</span>-values after single imputation are correct only when all predictions are made without error, which is unrealistic in practice. Rubin realised that replacing the missing value by <em>one</em> value cannot be correct in general. His solution was brilliant and straightforward: create multiple imputations that reflect the uncertainty of the unknown value.</p>
<p><span class="citation">Rubin (<a href="#ref-RUBIN1987" role="doc-biblioref">1987</a>)</span> describes the workflow in three steps:</p>
<ol style="list-style-type: decimal">
<li>Create <span class="math inline">\(m\)</span> completed datasets;</li>
<li>Estimate the quantities of scientific interest in each complete dataset;</li>
<li>Pool these estimates and their standard errors to a single result.</li>
</ol>
<p>The workflow produces estimates with known statistical properties under fairly general conditions.</p>
</div>
<div id="practical-problems-in-multivariate-imputation" class="section level2">
<h2>Practical problems in multivariate imputation</h2>
<p>In practice, missing data can appear everywhere in the data. The MICE algorithm handles multivariate missing data problems. This section highlights some of the practical problems that we need to address.</p>
<p>Let <span class="math inline">\(Y\)</span> denote the <span class="math inline">\(n \times p\)</span> matrix containing the data values on <span class="math inline">\(p\)</span> variables for all <span class="math inline">\(n\)</span> units in the sample. We define the <em>response indicator</em> <span class="math inline">\(R\)</span> as a binary <span class="math inline">\(n \times p\)</span> matrix, where a “0” indicates a missing value. Symbol <span class="math inline">\(Y_j\)</span> is the <span class="math inline">\(j\)</span>’th column in <span class="math inline">\(Y\)</span>. Symbol <span class="math inline">\(Y_{-j}\)</span> indicates all columns in <span class="math inline">\(Y\)</span> except <span class="math inline">\(Y_j\)</span>. Symbols <span class="math inline">\(Y_j^\mathrm{obs}\)</span> and <span class="math inline">\(Y_j^\mathrm{mis}\)</span> refer to the observed and missing values in <span class="math inline">\(Y_j\)</span>, respectively.</p>
<p>The basic conditional imputation model <span class="math inline">\(P(Y_j^\mathrm{mis}|Y_j^\mathrm{obs}, Y_{-j}, R)\)</span> specifies the distribution of the missing values <span class="math inline">\(Y_j^\mathrm{mis}\)</span> conditional on the observed data in <span class="math inline">\(Y_j^\mathrm{obs}\)</span>, on the remaining data <span class="math inline">\(Y_j\)</span> and on the response indicator <span class="math inline">\(R\)</span>. If we assume that the missing data are missing at random (MAR), then <span class="math inline">\(R\)</span> drops out of the model. The rationale for conditioning on <span class="math inline">\(Y_{-j}\)</span> is that this preserves the relations among the variables in the imputed data.</p>
<p><span class="citation">van Buuren (<a href="#ref-VANBUUREN2018" role="doc-biblioref">2018</a>)</span> highlighted various practical problems that occur:</p>
<ul>
<li>The predictors <span class="math inline">\(Y_{-j}\)</span> themselves can contain missing values;</li>
<li>“Circular” dependence can occur, where <span class="math inline">\(Y_j^\mathrm{mis}\)</span> depends on <span class="math inline">\(Y_h^\mathrm{mis}\)</span>, and <span class="math inline">\(Y_h^\mathrm{mis}\)</span> depends on <span class="math inline">\(Y_j^\mathrm{mis}\)</span> with <span class="math inline">\(h \neq j\)</span>, because in general <span class="math inline">\(Y_j\)</span> and <span class="math inline">\(Y_h\)</span> are correlated, even given other variables;</li>
<li>Variables are often of different types (e.g., binary, unordered, ordered, continuous), thereby making the application of theoretically convenient models, such as the multivariate normal, theoretically inappropriate;</li>
<li>Especially with large <span class="math inline">\(p\)</span> and small <span class="math inline">\(n\)</span>, collinearity or empty cells can occur;</li>
<li>The ordering of the rows and columns can be meaningful, e.g., as in longitudinal data;</li>
<li>The relation between <span class="math inline">\(Y_j\)</span> and predictors <span class="math inline">\(Y_{-j}\)</span> can be complicated, e.g., nonlinear, or subject to censoring processes;</li>
<li>Imputation can create impossible combinations, such as pregnant fathers.</li>
</ul>
<p>This list is by no means exhaustive, and other complexities may appear
for detailed data.</p>
</div>
<div id="sec:MICE" class="section level2">
<h2>The MICE algorithm</h2>
<p>The MICE algorithm provides an iterative solution to these problems. The procedure consists of the following steps:</p>
<ol style="list-style-type: decimal">
<li>Specify an imputation model <span class="math inline">\(P(Y_j^\mathrm{mis}|Y_j^\mathrm{obs}, Y_{-j}, R)\)</span> for variable <span class="math inline">\(Y_j\)</span> with <span class="math inline">\(j=1,\dots,p\)</span>.</li>
<li>For each <span class="math inline">\(j\)</span>, fill in starting imputations <span class="math inline">\(\dot Y_j^0\)</span> by random draws from <span class="math inline">\(Y_j^\mathrm{obs}\)</span>.</li>
<li>Repeat for <span class="math inline">\(t = 1,\dots,T\)</span>.</li>
<li>Repeat for <span class="math inline">\(j = 1,\dots,p\)</span>.</li>
<li>Define <span class="math inline">\(\dot Y_{-j}^t = (\dot Y_1^t,\dots,\dot Y_{j-1}^t,\dot Y_{j+1}^{t-1},\dots,\dot Y_p^{t-1})\)</span> as the currently complete data except <span class="math inline">\(Y_j\)</span>.</li>
<li>Draw <span class="math inline">\(\dot\phi_j^t \sim P(\phi_j^t|Y_j^\mathrm{obs}, \dot Y_{-j}^t, R)\)</span>.</li>
<li>Draw imputations <span class="math inline">\(\dot Y_j^t \sim P(Y_j^\mathrm{mis}|Y_j^\mathrm{obs}, \dot Y_{-j}^t, R, \dot\phi_j^t)\)</span>.</li>
<li>End repeat <span class="math inline">\(j\)</span>.</li>
<li>End repeat <span class="math inline">\(t\)</span>.</li>
</ol>
<p>The algorithm starts with a random draw from the observed data and imputes the incomplete data in a variable-by-variable fashion. One iteration consists of one cycle through all <span class="math inline">\(Y_j\)</span>. The number of iterations <span class="math inline">\(T\)</span> can often be low, say 5 or 10. The MICE algorithm generates multiple imputations by executing the procedure in parallel <span class="math inline">\(m\)</span> times.</p>
</div>
<div id="methodology" class="section level2">
<h2>Methodology</h2>
<div id="mcmc-conditions" class="section level3">
<h3>MCMC Conditions</h3>
<p>The MICE algorithm is a Markov chain Monte Carlo (MCMC) method, where the state space is the collection of all imputed values. In order to converge to a stationary distribution, a Markov chain needs to satisfy three critical conditions <span class="citation">(Roberts <a href="#ref-ROBERTS1996" role="doc-biblioref">1996</a>; Tierney <a href="#ref-TIERNEY1996" role="doc-biblioref">1996</a>)</span>:</p>
<ul>
<li><em>irreducible</em>, the chain must be able to reach all interesting
parts of the state space;</li>
<li><em>aperiodic</em>, the chain should not oscillate between different
states;</li>
<li><em>recurrence</em>, all interesting parts can be reached infinitely
often, at least from almost all starting points.</li>
</ul>
<p>Do these properties hold for the MICE algorithm? Irreducibility is generally not a problem since the user has considerable control over the state space. This flexibility is the main attraction of the MICE algorithm.</p>
<p>Periodicity is a potential problem and can arise in a situation where imputation models are inconsistent. A rather artificial example of an oscillatory behavior occurs when <span class="math inline">\(Y_1\)</span> is imputed by <span class="math inline">\(Y_2\beta+\epsilon_1\)</span> and <span class="math inline">\(Y_2\)</span> is imputed by <span class="math inline">\(-Y_1\beta+\epsilon_2\)</span> for some fixed, nonzero <span class="math inline">\(\beta\)</span>. The sampler will oscillate between two qualitatively different states, so the correlation between <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> after imputing <span class="math inline">\(Y_1\)</span> will differ from that after imputing <span class="math inline">\(Y_2\)</span>. In general, we would like the statistical inferences to be independent of the stopping point. A way to diagnose the <em>ping-pong</em> problem, or <em>order effect</em>, is to stop the chain at different points. The stopping point should not affect statistical inferences. The addition of noise to create imputations is a safeguard against periodicity and allows the sampler to “break out” more easily.</p>
<p>Non-recurrence may also be a potential difficulty, manifesting itself as explosive or non-stationary behaviour. For example, if imputations are created by deterministic functions, the Markov chain may lock up. We may diagnose such from the trace lines of the sampler. As long as we estimate the parameters of imputation models from the data, non-recurrence is mild or absent.</p>
</div>
<div id="compatibility" class="section level3">
<h3>Compatibility</h3>
<p>Gibbs sampling exploits the idea that knowledge of the conditional distributions is sufficient to determine a joint distribution if it exists. The convergence of the MICE algorithm to a (multivariate) joint distribution can be guaranteed when are conditions are known to be <em>compatible</em>. For example, when conditional regressions are all linear with a normal residual, the joint corresponds to the multivariate normal distribution.</p>
<p>There is active literature on compatibility. We refer to <span class="citation">van Buuren (<a href="#ref-VANBUUREN2018" role="doc-biblioref">2018</a>)</span> for a more extensive discussion of the topic. <span class="citation">van Buuren et al. (<a href="#ref-VANBUUREN2006" role="doc-biblioref">2006</a>)</span> described a small simulation study using strongly incompatible models. The adverse effects on the estimates after multiple imputation were only minimal in the cases studied. These simulations suggested that the results may be robust against violations of compatibility. <span class="citation">Li, Yu, and Rubin (<a href="#ref-LI2012" role="doc-biblioref">2012</a>)</span> presented three examples of problems with MICE. However, their examples differ from the usual sequential regression set up in various ways and do not undermine the validity of the approach <span class="citation">(Zhu and Raghunathan <a href="#ref-ZHU2015" role="doc-biblioref">2015</a>)</span>. <span class="citation">Liu et al. (<a href="#ref-LIU2013" role="doc-biblioref">2013</a>)</span> pointed out that application of incompatible conditional models cannot provide imputations from any joint model. However, they also found that Rubin’s rules provide consistent point estimates for incompatible models under fairly general conditions, as long as each conditional model was correctly specified. <span class="citation">Zhu and Raghunathan (<a href="#ref-ZHU2015" role="doc-biblioref">2015</a>)</span> showed that incompatibility does not need to lead to divergence. While there is no joint model to converge to, the algorithm can still converge. The key to achieving convergence is that the imputation models should closely model the data. For example, include the skewness of the residuals, or ideally, generate the imputations from the underlying (but usually unknown) mechanism that generated the data.</p>
<p>In the majority of cases, scientific interest will focus on quantities
that are more remote to the joint density, such as regression weights,
factor loadings, and prevalence estimates. In such cases, the
joint distribution is more like a nuisance factor that has no intrinsic
value.</p>
<p>Apart from potential feedback problems, it appears that incompatibility
seems like a relatively minor problem in practice, especially if the
missing data rate is modest, and if the imputation models fit the data well.
In order to evaluate these aspects, we need to inspect convergence and
assess the fit of the imputations.</p>
</div>
<div id="sec:howlarget" class="section level3">
<h3>Number of iterations</h3>
<p>When we calculate <span class="math inline">\(m\)</span> sampling streams in parallel, we may monitor convergence by plotting one or more statistics of interest in each stream against iteration number <span class="math inline">\(t\)</span>. Common statistics to be plotted are the mean and standard deviation of the synthetic data, as well as the correlation between different variables. The pattern should be free of a trend, and the variance within a chain should approximate the variance between chains.</p>
<p>In practice, a low number of iterations appears to be enough. <span class="citation">Brand (<a href="#ref-BRAND1999" role="doc-biblioref">1999</a>)</span> and <span class="citation">van Buuren, Boshuizen, and Knook (<a href="#ref-VANBUUREN1999" role="doc-biblioref">1999</a>)</span> set the number of iterations <span class="math inline">\(T\)</span> relatively low, usually somewhere between 5 to 20 iterations. This number is much lower than in other applications of MCMC methods, which often require thousands of iterations. The imputations form the only memory in the MICE algorithm. Note that the imputed data can have a considerable amount of random noise, depending on the strength of the relations between the variables. Applications of MICE with lowly correlated data, therefore inject much noise into the system. Hence, the autocorrelation over <span class="math inline">\(t\)</span> will be low, and convergence will be rapid, and in fact, immediate if all variables are independent. Thus, the incorporation of noise into the imputed data has the side-effect of speeding up convergence. Reversely, situations to watch out for occur if:</p>
<ul>
<li>the correlations between the <span class="math inline">\(Y_j\)</span> are high;</li>
<li>the missing data rates are high; or</li>
<li>constraints on parameters across different variables exist.</li>
</ul>
<p>The first two conditions directly affect the amount of autocorrelation in the system. The latter condition becomes relevant for customised imputation models.</p>
<p>A recent simulation study by <span class="citation">Oberman, van Buuren, and Vink (<a href="#ref-OBERMAN2020" role="doc-biblioref">2020</a>)</span> found that conventional convergence diagnostics like <span class="math inline">\(\hat R\)</span> <span class="citation">(Gelman and Rubin <a href="#ref-GELMAN1991" role="doc-biblioref">1991</a>)</span> are too conservative for missing data imputation. When these diagnostics typically indicate convergence after only 30-40 iterations, the parameters estimates achieve their statistical properties between 5 and 10 iterations. It is, however, important not to rely automatically on this result as some applications can require considerably more iterations.</p>
</div>
</div>
<div id="performance" class="section level2">
<h2>Performance</h2>
<p>The MICE algorithm is extremely flexible as it allows to user to set each conditional density. Most software packages provide reasonable defaults for everyday situations, so the actual effort required from the user may be small. However, it generally pays off to go beyond the default to address particular features in the data or science, like derived variables, interaction terms, skipping pattern, multi-level data and time dependencies.</p>
<p>Many simulation studies provide evidence that MICE algorithm, or similar methodologies, generally yields estimates that are unbiased and that possess appropriate coverage <span class="citation">(Brand <a href="#ref-BRAND1999" role="doc-biblioref">1999</a>; Raghunathan et al. <a href="#ref-RAGHUNATHAN2001" role="doc-biblioref">2001</a>; Brand et al. <a href="#ref-BRAND2003" role="doc-biblioref">2003</a>; Tang et al. <a href="#ref-TANG2005" role="doc-biblioref">2005</a>; van Buuren et al. <a href="#ref-VANBUUREN2006" role="doc-biblioref">2006</a>; Horton and Kleinman <a href="#ref-HORTON2007" role="doc-biblioref">2007</a>; Yu, Burton, and Rivero-Arias <a href="#ref-YU2007" role="doc-biblioref">2007</a>)</span>. <span class="citation">Nair et al. (<a href="#ref-NAIR2013" role="doc-biblioref">2013</a>)</span> summarise their results as</p>
<blockquote>
<p>We observe that MICE is overall the best imputation algorithm.</p>
</blockquote>
</div>
<div id="future-work" class="section level2">
<h2>Future work</h2>
<p>We may extend the MICE algorithm in various ways.</p>
<div id="skipping-imputations-and-overimputation" class="section level3">
<h3>Skipping imputations and overimputation</h3>
<p>By default, the MICE algorithm imputes all missing data and leaves the observed data untouched. In some cases, it may also be useful to skip imputation of specific cells. For example, we wish to skip imputation of quality of life for the deceased, or not impute customer satisfaction for people who did not buy the product. The primary difficulty with this option is that it creates missing data in the predictors, so the imputer should either remove the predictor from all imputation models or have the missing values propagated through the algorithm. Another use case involves imputing cells with observed data, a technique called <em>overimputation</em>. For example, it may be useful to evaluate whether the observed point data fit the imputation model. If all is well, we expect the observed data point in the centre of the multiple imputations. The primary difficulty with this option is to ensure that we use only the observed data (and not the imputed data) as an outcome in the imputation model. Version <code>3.0</code> of <code>mice</code> includes the <code>where</code> argument. The specification is a matrix with binary values that has the same dimensions as the data, that indicates where MICE should create imputations. We may use this matrix to specify for each cell, whether it should be imputed or not. The default is that the missing data are imputed.</p>
</div>
<div id="sec:blockvar" class="section level3">
<h3>Blocks of variables, hybrid imputation</h3>
<p>The MICE algorithm imputes each variable separately. In some cases, it is useful to impute multiple values simultaneously, as a block. In actual data analysis sets of variables are often connected in some way. Examples are:</p>
<ul>
<li>A set of scale items and its total score;</li>
<li>A variable with one or more transformations;</li>
<li>Two variables with one or more interaction terms;</li>
<li>A block of normally distributed <span class="math inline">\(Z\)</span>-scores;</li>
<li>Compositions that add up to a total;</li>
<li>Set of variables that are collected together.</li>
</ul>
<p>Instead of specifying the steps for each variable separately, it is more user-friendly to impute these as a block. Version <code>3.0</code> of <code>mice</code> includes a new <code>block</code> argument that partitions the complete set of variables into blocks. All variables within the same block are jointly imputed. The joint models need to be open to accepting external covariates. One possibility is to use predictive mean matching to impute multivariate nonresponse, where the donor values for the variables within the block come from the same donor <span class="citation">(Little <a href="#ref-LITTLE1988" role="doc-biblioref">1988</a>)</span>. The main algorithm in <code>mice 3.0</code> iterates over the blocks rather than the variables. By default, each variable is a block, which gives normal behaviour.</p>
</div>
<div id="sec:blockunit" class="section level3">
<h3>Blocks of units, monotone blocks</h3>
<p>Another way to partition the data is to define blocks of units. One weakness of the MICE algorithm is that it may become unstable when many of the predictors are imputed. <span class="citation">Zhu (<a href="#ref-ZHU2016" role="doc-biblioref">2016</a>)</span> developed a solution called “Block sequential regression multivariate imputation”, which partitions units into blocks according to the missing data pattern. The imputation model for a given variable is modified for each block, such that only the observed data with the block can serve as a predictor. The method generalises the monotone block approach of <span class="citation">Li et al. (<a href="#ref-LI2014" role="doc-biblioref">2014</a>)</span>.</p>
</div>
<div id="separate-training-from-test-data" class="section level3">
<h3>Separate training from test data</h3>
<p>The MICE algorithm uses all rows in the data to estimate and apply the imputation model. In practice, we sometimes wish to estimate the imputation model on one dataset and apply it to another. The <code>ignore</code> argument to the <code>mice()</code> function specifies the set of rows that MICE will ignore when creating the imputation model. The default is to include all rows. We may use the feature to split the data into a training set (on which the imputation model is built) and a test set (that does not influence the imputation model estimates). The feature is still experimental but is likely to attract interest from the data science and machine learning communities.</p>
</div>
</div>
<div id="conclusion" class="section level2">
<h2>Conclusion</h2>
<p>Multivariate missing data lead to analytic problems caused by mutual dependencies between incomplete variables. For general missing data patterns, the MICE algorithm is a flexible and straightforward procedure that allows for imputed values close to the data.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-BRAND1999">
<p>Brand, J. P. L. 1999. “Development, Implementation and Evaluation of Multiple Imputation Strategies for the Statistical Analysis of Incomplete Data Sets.” PhD thesis, Rotterdam: Erasmus University.</p>
</div>
<div id="ref-BRAND2003">
<p>Brand, J. P. L., S. van Buuren, C. G. M. Groothuis-Oudshoorn, and E. S. Gelsema. 2003. “A Toolkit in SAS for the Evaluation of Multiple Imputation Methods.” <em>Statistica Neerlandica</em> 57 (1): 36–45.</p>
</div>
<div id="ref-GELMAN2004">
<p>Gelman, A. 2004. “Parameterization and Bayesian Modeling.” <em>Journal of the American Statistical Association</em> 99 (466): 537–45.</p>
</div>
<div id="ref-GELMAN1991">
<p>Gelman, A., and D. B. Rubin. 1991. “Inference from Iterative Simulation Using Multiple Sequences (with Discussion).” <em>Statistical Science</em> 7: 457–511.</p>
</div>
<div id="ref-HECKERMAN2001">
<p>Heckerman, D., D. M. Chickering, C. Meek, R. Rounthwaite, and C. Kadie. 2001. “Dependency Networks for Inference, Collaborative Filtering, and Data Visualisation.” <em>Journal of Machine Learning Research</em> 1 (1): 49–75.</p>
</div>
<div id="ref-HORTON2007">
<p>Horton, N. J., and K. P. Kleinman. 2007. “Much Ado About Nothing: A Comparison of Missing Data Methods and Software to Fit Incomplete Data Regression Models.” <em>The American Statistician</em> 61 (1): 79–90.</p>
</div>
<div id="ref-KENNICKELL1991">
<p>Kennickell, A. B. 1991. “Imputation of the 1989 Survey of Consumer Finances: Stochastic Relaxation and Multiple Imputation.” <em>ASA 1991 Proceedings of the Section on Survey Research Methods</em>, 1–10.</p>
</div>
<div id="ref-LI2014">
<p>Li, F., M. Baccini, F. Mealli, E. R. Zell, C. E. Frangakis, and D. B. Rubin. 2014. “Multiple Imputation by Ordered Monotone Blocks with Application to the Anthrax Vaccine Research Program.” <em>Journal of Computational and Graphical Statistics</em> 23 (3): 877–92.</p>
</div>
<div id="ref-LI2012">
<p>Li, F., Y. Yu, and D. B. Rubin. 2012. “Imputing Missing Data by Fully Conditional Models: Some Cautionary Examples and Guideline.” <em>Duke University Department of Statistical Science Discussion Paper</em> 11-24.</p>
</div>
<div id="ref-LITTLE1988">
<p>Little, R. J. A. 1988. “Missing-Data Adjustments in Large Surveys (with Discussion).” <em>Journal of Business Economics and Statistics</em> 6 (3): 287–301.</p>
</div>
<div id="ref-LIU2013">
<p>Liu, J., A. Gelman, J. Hill, Y. S. Su, and J. Kropko. 2013. “On the Stationary Distribution of Iterative Imputations.” <em>Biometrika</em> 101 (1): 155–73.</p>
</div>
<div id="ref-NAIR2013">
<p>Nair, V., R. Kidambi, S. Sellamanickam, S. S. Keerthi, J. Gehrke, and V. Narayanan. 2013. “A Quantitative Evaluation Framework for Missing Value Imputation Algorithms.” <em>arXiv Preprint arXiv:1311.2276</em>.</p>
</div>
<div id="ref-OBERMAN2020">
<p>Oberman, H. I., S. van Buuren, and G. Vink. 2020. “Missing the Point: Non-Convergence in Iterative Imputation Algorithms.” <em>First Workshop on the Art of Learning with Missing Values (Artemiss) Hosted by the 37th International Conference on Machine Learning (ICML)</em>.</p>
</div>
<div id="ref-RAGHUNATHAN2001">
<p>Raghunathan, T. E., J. M. Lepkowski, J. Van Hoewyk, and P. W. Solenberger. 2001. “A Multivariate Technique for Multiply Imputing Missing Values Using a Sequence of Regression Models.” <em>Survey Methodology</em> 27 (1): 85–95.</p>
</div>
<div id="ref-ROBERTS1996">
<p>Roberts, G. O. 1996. “Markov Chain Concepts Related to Sampling Algorithms.” In <em>Markov Chain Monte Carlo in Practice</em>, edited by W. R. Gilks, S. Richardson, and D. J. Spiegelhalter, 45–57. London: Chapman &amp; Hall.</p>
</div>
<div id="ref-RUBIN1987">
<p>Rubin, D. B. 1987. <em>Multiple Imputation for Nonresponse in Surveys</em>. New York: John Wiley &amp; Sons.</p>
</div>
<div id="ref-RUBIN2003">
<p>Rubin, D. 2003. “Nested Multiple Imputation of NMES via Partially Incompatible MCMC.” <em>Statistica Neerlandica</em> 57 (1): 3–18.</p>
</div>
<div id="ref-TANG2005">
<p>Tang, L., J. Unüntzer, J. Song, and T. R. Belin. 2005. “A Comparison of Imputation Methods in a Longitudinal Randomized Clinical Trial.” <em>Statistics in Medicine</em> 24 (14): 2111–28.</p>
</div>
<div id="ref-TIERNEY1996">
<p>Tierney, L. 1996. “Introduction to General State-Space Markov Chain Theory.” In <em>Markov Chain Monte Carlo in Practice</em>, edited by W. R. Gilks, S. Richardson, and D. J. Spiegelhalter, 59–74. London: Chapman &amp; Hall.</p>
</div>
<div id="ref-VANBUUREN2011">
<p>van Buuren, S. 2011. “Multiple Imputation of Multilevel Data.” In <em>The Handbook of Advanced Multilevel Analysis</em>, edited by J. J. Hox and J. K. Roberts, 173–96. Milton Park, UK: Routledge.</p>
</div>
<div id="ref-VANBUUREN2018">
<p>van Buuren, S. 2018. <em>Flexible Imputation of Missing Data. Second Edition.</em> Boca Raton, FL: Chapman &amp; Hall/CRC.</p>
</div>
<div id="ref-VANBUUREN1999">
<p>van Buuren, S., H. C. Boshuizen, and D. L. Knook. 1999. “Multiple Imputation of Missing Blood Pressure Covariates in Survival Analysis.” <em>Statistics in Medicine</em> 18 (6): 681–94.</p>
</div>
<div id="ref-VANBUUREN2006">
<p>van Buuren, S., J. P. L. Brand, C. G. M. Groothuis-Oudshoorn, and D. B. Rubin. 2006. “Fully Conditional Specification in Multivariate Imputation.” <em>Journal of Statistical Computation and Simulation</em> 76 (12): 1049–64.</p>
</div>
<div id="ref-VANBUUREN1999B">
<p>van Buuren, S., and C. G. M. Groothuis-Oudshoorn. 1999. “Flexible Multivariate Imputation by MICE.” PG/VGZ/99.054. Leiden: TNO Prevention; Health.</p>
</div>
<div id="ref-YU2007">
<p>Yu, L-M., A. Burton, and O. Rivero-Arias. 2007. “Evaluation of Software for Multiple Imputation of Semi-Continuous Data.” <em>Statistical Methods in Medical Research</em> 16 (3): 243–58.</p>
</div>
<div id="ref-ZHU2016">
<p>Zhu, J. 2016. “Assessment and Improvement of a Sequential Regression Multivariate Imputation Algorithm.” PhD thesis, University of Michigan.</p>
</div>
<div id="ref-ZHU2015">
<p>Zhu, J., and T. E. Raghunathan. 2015. “Convergence Properties of a Sequential Regression Multiple Imputation Algorithm.” <em>Journal of the American Statistical Association</em> 110 (511): 1112–24.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>

<a href="references.html" class="navigation navigation-next navigation-unique" aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["wileystatsref.pdf", "wileystatsref.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
